{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wektory słów\n",
    "\n",
    "W ramach drugiej części ćwiczeń przyjrzymy się gęstej reprezentacji semantycznej, czyli wektorom słów (ang. *word embeddings*):\n",
    "\n",
    "Ćwiczenia wykonamy przy użyciu znakomitej biblioteki [SpaCy](http://spacy.io). W ćwiczeniach wykorzystamy zarówno mode języka angielskiego, jak i języka polskiego. W celu dokładniejszego przyjrzenia się działaniu wektorów proponuję wykorzystać największy model językowy dla języka polskiego.\n",
    "\n",
    "```bash\n",
    "python -m spacy download pl_core_news_lg\n",
    "\n",
    "pythom -m spacy validate\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T15:04:45.729956Z",
     "start_time": "2020-12-03T15:04:12.772181Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('pl_core_news_lg')\n",
    "\n",
    "allWords = [\n",
    "    orth\n",
    "    for orth in nlp.vocab.vectors \n",
    "    if nlp.vocab[orth].has_vector\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wprowadzenie do wektorów słów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T15:04:49.960539Z",
     "start_time": "2020-12-03T15:04:49.918845Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp('król').vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T15:04:52.199034Z",
     "start_time": "2020-12-03T15:04:52.061482Z"
    }
   },
   "outputs": [],
   "source": [
    "words = ['król','królik','cesarz','kier','lew','Wojtuś','Sławomir']\n",
    "\n",
    "for w in words:\n",
    "    print(f\"{w:>12}: {nlp('król').similarity(nlp(w))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poniższa funkcja pozwoli nam na przeszukanie całego korpusu słów i znalezienie top-n najbardziej podobnych słów do danego wektora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T15:08:56.307362Z",
     "start_time": "2020-12-03T15:08:56.301660Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def find_similar_vectors(vec, topn=10, tabu=list(), only_lowercase=True):\n",
    "\n",
    "    def _cosine(v1, v2): return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "\n",
    "    results = [\n",
    "        w\n",
    "        for w in tqdm(allWords)\n",
    "        if nlp.vocab[w].text not in tabu\n",
    "        and nlp.vocab[w].is_lower == only_lowercase\n",
    "        and _cosine(nlp.vocab[w].vector, vec) > 0.5\n",
    "    ]\n",
    "    \n",
    "    results.sort(key=lambda w: _cosine(nlp.vocab[w].vector, vec), reverse=True)\n",
    "\n",
    "    for r in results[:topn]:\n",
    "        print(f'{nlp.vocab[r].text:>15}: {_cosine(nlp.vocab[r].vector, vec):.8f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T15:06:58.176678Z",
     "start_time": "2020-12-03T15:06:49.574146Z"
    }
   },
   "outputs": [],
   "source": [
    "v = nlp('król').vector\n",
    "\n",
    "find_similar_vectors(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wektory mogą posłużyć do znajdowania analogii między słowami:\n",
    "\n",
    "$$ \\mathit{król} - \\mathit{mężczyzna} = \\mathit{???} - \\mathit{kobieta} $$\n",
    "\n",
    "$$ \\mathit{???} = \\mathit{król} - \\mathit{mężczyzna} + \\mathit{kobieta} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T15:09:42.439509Z",
     "start_time": "2020-12-03T15:09:33.311706Z"
    }
   },
   "outputs": [],
   "source": [
    "v = nlp('król').vector - nlp('mężczyzna').vector + nlp('kobieta').vector\n",
    "\n",
    "find_similar_vectors(v, tabu=['król','mężczyzna','kobieta'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### zadanie samodzielne\n",
    "\n",
    "Wykonaj odpowiednie działania na wektorach słów aby znaleźć ukryte słowa:\n",
    "\n",
    "- Niemcy - Berlin = Rosja - ?\n",
    "- ? = czerwony + żółty\n",
    "- tygrys - kot = wilk - ?\n",
    "- jem - zjadłem = idę - ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wykorzystanie wektorów słów"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W poniższym przykładzie wykorzystamy technikę grupowania (ang. *clustering*) do automatycznego pogrupowania zdań pochodzących z [artykułu o Poznaniu](https://pl.wikipedia.org/wiki/Pozna%C5%84)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T16:27:56.777306Z",
     "start_time": "2020-12-03T16:27:56.085141Z"
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from wordfreq import zipf_frequency\n",
    "\n",
    "import requests\n",
    "import re\n",
    "\n",
    "respond = requests.get(\"https://pl.wikipedia.org/wiki/Poznań\")\n",
    "soup = BeautifulSoup(respond.text, \"html\")\n",
    "page = soup.find_all('p')\n",
    "\n",
    "raw_text = ''.join([re.sub(r'\\[\\d+\\]', '', paragraph.text) for paragraph in page])\n",
    "raw_text = raw_text.replace(',', ' ').replace('(', ' ').replace(')', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T16:29:06.573932Z",
     "start_time": "2020-12-03T16:28:12.810884Z"
    }
   },
   "outputs": [],
   "source": [
    "words = {\n",
    "    word.lower(): nlp(word).vector\n",
    "    for word in tqdm(set(raw_text.split()))\n",
    "    if zipf_frequency(word, 'pl') < 5\n",
    "    and zipf_frequency(word, 'pl') > 0\n",
    "    and np.any(nlp(word).vector)\n",
    "}\n",
    "\n",
    "print(f\"Liczba unikalnych słów w artykule o Poznaniu: {len(words)}\")\n",
    "\n",
    "word_vectors = np.vstack(list(words.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T16:33:38.062697Z",
     "start_time": "2020-12-03T16:33:36.036567Z"
    }
   },
   "outputs": [],
   "source": [
    "k = 5\n",
    "\n",
    "model = KMeans(n_clusters=k, max_iter=1000)\n",
    "model.fit(word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T16:34:03.441267Z",
     "start_time": "2020-12-03T16:34:03.388501Z"
    }
   },
   "outputs": [],
   "source": [
    "for i, c in enumerate(model.cluster_centers_):\n",
    "\n",
    "    cluster_words = sorted(words.items(), key=lambda x: np.dot(x[1], c), reverse=True)\n",
    "    print(f\"Temat {i}: {[k for k,v in cluster_words][:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### zadanie samodzielne\n",
    "\n",
    "Napisz funkcję, która dla podanego słowa znajdzie zbiór *k* najmniej podobnych słów. Opcjonalnie: postaraj się ograniczyć zbiór rozważanych słów tylko do słów które stanowią tę samą część mowy co podane słowo, tj. dla słowa \"biały\" poszukaj tylko najmniej podobnych przymiotników.\n",
    "\n",
    "Wynik powinien wyglądać mniej więcej tak:\n",
    "\n",
    "```python\n",
    "find_dissimilar_words('dzień')\n",
    "```\n",
    "\n",
    "    vitara: -0.27556160\n",
    "    elka: -0.26526517\n",
    "    tesli: -0.26290190\n",
    "    zippo: -0.25923923\n",
    "    panasonica: -0.25569871\n",
    "    durnia: -0.23886403\n",
    "    demka: -0.23708199\n",
    "    babola: -0.23590432\n",
    "    bina: -0.23532979\n",
    "    virago: -0.23311970"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wektory zdań"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W tym ćwiczeniu wykorzystamy dostarczony przez Google [Universal Sentence Encoder](https://tfhub.dev/google/universal-sentence-encoder/4). W celu przeprowadzenia ćwiczenia konieczne jest zainstalowanie wersji biblioteki `tensorflow` powyżej 1.15.\n",
    "\n",
    "```bash\n",
    "pip install \"tensorflow>=1.15,<2.0\"\n",
    "\n",
    "pip install tensorflow_hub\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "import re, os, csv\n",
    "\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T21:31:38.452106Z",
     "start_time": "2020-12-03T21:31:38.397303Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('godzilla.srt') as f:\n",
    "    phrases = f.readlines()\n",
    "\n",
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "\n",
    "def remove_tags(text):\n",
    "    return TAG_RE.sub('', text)\n",
    "\n",
    "phrases = list(map(remove_tags, phrases))\n",
    "phrases = [ phrase.rstrip() for phrase in phrases if len(phrase.split()) > 3 ]\n",
    "\n",
    "annotations = [\n",
    "    'we are going to die',\n",
    "    'we will all die',\n",
    "    \"i don't want to die\",\n",
    "    'we will not survive'\n",
    "]\n",
    "\n",
    "print(f'Długość listy dialogowej filmu: {len(phrases)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_input_placeholder = tf.placeholder(tf.string, shape=(None))\n",
    "annotation_input_placeholder = tf.placeholder(tf.string, shape=(None))\n",
    "\n",
    "phrase_encoding = tf.nn.l2_normalize(embed(phrase_input_placeholder), dim=1)\n",
    "annotation_encoding = tf.nn.l2_normalize(embed(annotation_input_placeholder), dim=1)\n",
    "\n",
    "similarity_score = tf.matmul(phrase_encoding, annotation_encoding, adjoint_b=True)\n",
    "\n",
    "similarity_threshold = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as session:\n",
    "    \n",
    "    session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "    \n",
    "    phrase_embeddings_, annotation_embeddings_, cosine_dist = session.run(\n",
    "        [phrase_encoding, annotation_encoding, similarity_score],\n",
    "        feed_dict = {\n",
    "            phrase_input_placeholder: phrases,\n",
    "            annotation_input_placeholder: annotations\n",
    "        })\n",
    "    \n",
    "    for i, phrase_embedding in enumerate(phrase_embeddings_):\n",
    "        for j, annotation_embedding in enumerate(annotation_embeddings_):\n",
    "            if cosine_dist[i][j] > similarity_threshold:\n",
    "                print(f'PHRASE: {phrases[i]:<40} ANNOTATION: {annotations[j]:<30} distance={cosine_dist[i][j]:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
