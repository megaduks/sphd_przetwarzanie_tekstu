{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tradycyjna analiza tekstu\n",
    "\n",
    "W ramach pierwszej części ćwiczeń przyjrzymy się tradycyjnym metodom analizy danych tekstowych i zapoznamy się z technikami:\n",
    "\n",
    "- tokenizacji\n",
    "- lematyzacji\n",
    "- rozkładu semantycznego\n",
    "- reprezentacji TF-IDF\n",
    "\n",
    "Ćwiczenia wykonamy przy użyciu znakomitej biblioteki [SpaCy](http://spacy.io). Konieczne jest zatem zainstalowanie biblioteki oraz ściągnięcie modelu języka. W ćwiczeniach wykorzystamy zarówno mode języka angielskiego, jak i języka polskiego. Przed przystąpieniem do realizacji ćwiczenia niezbędne jest wykonanie następujących kroków:\n",
    "\n",
    "```bash\n",
    "pip install -U spacy\n",
    "\n",
    "python -m spacy download en_core_web_md\n",
    "\n",
    "python -m spacy download pl_core_news_md\n",
    "\n",
    "python -m spacy validate\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizacja i lematyzacja\n",
    "\n",
    "W pierwszym kroku załadujemy model języka i przetworzymy tekst dokonując podziału na tokeny, a następnie sprawdzimy, jakie informacje możemy wyciągnąć z każdego tokenu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T12:31:03.898861Z",
     "start_time": "2020-12-03T12:30:40.896938Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('pl_core_news_md')\n",
    "\n",
    "# lista wszystkich słów które posiadają zdefiniowane wektory\n",
    "allWords = [\n",
    "    orth\n",
    "    for orth in nlp.vocab.vectors \n",
    "    if nlp.vocab[orth].has_vector\n",
    "]\n",
    "\n",
    "print(f\"Liczba słów w słowniku: {len(allWords)}\")\n",
    "print(f\"Słownik: {[nlp.vocab[orth].text for orth in allWords[:10]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T12:33:04.140729Z",
     "start_time": "2020-12-03T12:33:04.098456Z"
    }
   },
   "outputs": [],
   "source": [
    "doc = 'Król Karol Ubogowłosy XII kupił królowej Karolinie pięć korali koloru koralowego na urodziny 1 grudnia!'\n",
    "\n",
    "for t in nlp(doc):\n",
    "    print(f\"{t.text:<12}: {t.lemma_:<10}: {t.pos_:<7}: {t.is_digit}: {t.is_punct}: {t.is_oov}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### zadanie samodzielne\n",
    "\n",
    "Zapoznaj się z [dokumentacją klasy Token](https://spacy.io/api/token) i dla poniższego zdania wyświetl, dla każdego tokenu, następujące informacje:\n",
    "\n",
    "- tekst tokenu\n",
    "- symbol części mowy (zarówno ogólny jak i szczegółowy)\n",
    "- znormalizowaną formę tokenu\n",
    "- formę słownikową tokenu\n",
    "- informację czy token jest cały napisany kapitalikami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T12:40:49.519871Z",
     "start_time": "2020-12-03T12:40:49.504429Z"
    }
   },
   "outputs": [],
   "source": [
    "doc = 'Rola Roberta DeNiro w tym filmie była NIESAMOWITA!!!'\n",
    "\n",
    "for t in nlp(doc):\n",
    "    print(f\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T12:41:59.135686Z",
     "start_time": "2020-12-03T12:41:59.104640Z"
    }
   },
   "outputs": [],
   "source": [
    "doc = 'na stronie www.amazon.com kupiłem 2 kindle (za 90,34 EUR) od morzy@gmail.com'\n",
    "\n",
    "for t in nlp(doc):\n",
    "    print(f\"{t.text:<17}: {t.like_url}: {t.like_email}: {t.like_num}: {t.is_left_punct}: {t.is_right_punct}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### zadanie samodzielne\n",
    "\n",
    "Przygotuj przykład zdania, które będzie zawierało następujące elementy:\n",
    "\n",
    "- fragment przypominający e-mail\n",
    "- fragment przypominający kwotę pieniężną\n",
    "- fragment przypominający numer NIP\n",
    "- słowo o zdecydowanie pozytywnym wydźwięku\n",
    "- słowo o zdecydowanie negatywnym wydźwięku\n",
    "\n",
    "a następnie wyświetl dla każdgo tokenu następujące informacje:\n",
    "\n",
    "- czy dany token przypomina e-mail?\n",
    "- czy dany token przypomina liczbę?\n",
    "- wydźwięk (sentyment) tokenu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T12:46:24.445647Z",
     "start_time": "2020-12-03T12:46:24.438220Z"
    }
   },
   "outputs": [],
   "source": [
    "doc = ''\n",
    "\n",
    "for t in nlp(doc):\n",
    "    print(f\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Ekstrakcja nazwanych encji\n",
    "\n",
    "NER (ang. named entity recognition) to zadanie NLP polegające na zidentyfikowaniu w tekście tokenów reprezentujących wystąpienia określonych kategorii obiektów świata rzeczywistego, np. osób, miejsc, organizacji, dat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T12:49:37.873091Z",
     "start_time": "2020-12-03T12:49:37.834405Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "doc = \"\"\"W poznańskiej Katedrze znajduje się \n",
    "symboliczny grobowiec pierwszych władców Polski:\n",
    "Mieszka I i Bolesława I Chrobrego. \n",
    "\"\"\"\n",
    "\n",
    "from spacy import displacy\n",
    "displacy.render(nlp(doc), style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### zadanie samodzielne\n",
    "\n",
    "Znajdź [schemat etykiet użyty w modelu języka](https://spacy.io/models/pl) (*hint*: szukaj Labeling Scheme) i napisz akapit, który będzie zawierał wystąpienie każdej encji ze schematu etykiet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T12:50:57.880991Z",
     "start_time": "2020-12-03T12:50:57.852600Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "doc = \"\"\"\n",
    "\"\"\"\n",
    "\n",
    "from spacy import displacy\n",
    "displacy.render(nlp(doc), style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rozkład syntaktyczny zdania\n",
    "\n",
    "Zadanie rozkładu syntaktycznego polega na zidentyfikowaniu ról, jaką poszczególne tokeny pełnią w zdaniu (identyfikacja części zdania), oraz znalezieniu zależności między tymi częściami."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T13:19:11.233843Z",
     "start_time": "2020-12-03T13:19:11.205570Z"
    }
   },
   "outputs": [],
   "source": [
    "doc = \"Czarna krowa w kropki bordo gryzła trawę kręcąc mordą.\"\n",
    "\n",
    "displacy.render(nlp(doc), style=\"dep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T13:22:48.411145Z",
     "start_time": "2020-12-03T13:22:48.381047Z"
    }
   },
   "outputs": [],
   "source": [
    "for t in nlp(doc):\n",
    "    print(f\"{t.text:<10} {t.dep_:<8} {t.head.text:<10} {t.head.pos_}   children: {[child for child in t.children]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### zadanie samodzielne\n",
    "\n",
    "Zapoznaj się z definicją poszczególnych [rodzajów zależności](https://universaldependencies.org/pl/index.html) a następnie postaraj się zidentyfikować w poniższym zdaniu pary słów reprezentujące podmiot i orzeczenie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T13:33:29.062960Z",
     "start_time": "2020-12-03T13:33:29.031184Z"
    }
   },
   "outputs": [],
   "source": [
    "doc = \"Jan powiedział, że chce obejrzeć film, ale Anna, niechętna temu pomysłowi, zaproponowała wyjście do kawiarni\"\n",
    "\n",
    "for t in nlp(doc):\n",
    "    if t.dep_ == 'xxx uzupełnij xxx':\n",
    "        print(f\"{t.text:<15} {t.dep_:<8}  {t.head.text:<15}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wektory TF-IDF i częstotliwość słów"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funkcja `zipf_frequency` pozwala na wyznaczenie, w pewnym przybliżeniu, częstości występowania danego słowa w języku. Wartość zwracana przez funkcję to logarytm częstości, słowa rzadkie mają wartość w przedziale [0-1], podczas gdy najpopularniejsze słowa (się, w, na, nie) mają wartośc w przedziale [7-8]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T13:43:09.113896Z",
     "start_time": "2020-12-03T13:43:09.092406Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from wordfreq import zipf_frequency\n",
    "\n",
    "for random_word in [nlp.vocab[allWords[i]].text for i in np.random.randint(len(allWords), size=10)]:\n",
    "    print(f\"{random_word}, {zipf_frequency(random_word,'pl')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poniższy kod wykorzystuje bibliotekę `BeautifulSoup` do pobrania całego opisu zamieszczonego na stronie [pl.wikipedia.org/wiki/Poznań](https://pl.wikipedia.org/wiki/Pozna%C5%84) Następnie tekst jest łączony do postaci jednego łańcucha znaków i usuwane są wszystkie odnośniki do literatury w postaci [1], [2], itd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T13:48:53.093666Z",
     "start_time": "2020-12-03T13:48:51.180601Z"
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "\n",
    "respond = requests.get(\"https://pl.wikipedia.org/wiki/Poznań\")\n",
    "soup = BeautifulSoup(respond.text, \"html\")\n",
    "page = soup.find_all('p')\n",
    "\n",
    "raw_text = ''.join([re.sub(r'\\[\\d+\\]', '', paragraph.text) for paragraph in page])\n",
    "\n",
    "print(raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W dalszej części ćwiczenia przyjmiemy, że jednostką analizy (dokumentem) jest jedno zdanie. Przyjrzymy się dwóm tradycyjnym sposobom reprezentacji zdań w przestrzeni wektorowej:\n",
    "\n",
    "- modelowi bag-of-words\n",
    "- modelowi tf-idf\n",
    "\n",
    "Do wyznaczenia wektorów wykorzystamy gotowe klasy z biblioteki `scikit-learn`. Wykorzystamy także gotową listę słów typu *stopword* dla języka polskiego dostępną pod [tym adresem](https://raw.githubusercontent.com/bieli/stopwords/master/polish.stopwords.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T14:08:24.785132Z",
     "start_time": "2020-12-03T14:08:22.505151Z"
    }
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/bieli/stopwords/master/polish.stopwords.txt\"\n",
    "stop_words = urllib.request.urlopen(url).read().decode('utf-8').split()\n",
    "\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T14:22:19.518186Z",
     "start_time": "2020-12-03T14:22:19.507797Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "phrases = raw_text.split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T14:22:26.441374Z",
     "start_time": "2020-12-03T14:22:26.403986Z"
    }
   },
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(min_df=0.01, max_df=0.1, stop_words=stop_words)\n",
    "\n",
    "count_vectorized_phrases = count_vectorizer.fit_transform(phrases)\n",
    "print(f\"Phrase: {phrases[3]}\")\n",
    "print(f\"Vector representation: {count_vectorized_phrases.todense()[3]}\")\n",
    "print(f\"Features: {count_vectorizer.get_feature_names()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T14:22:27.999825Z",
     "start_time": "2020-12-03T14:22:27.955563Z"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(min_df=0.01, max_df=0.1, stop_words=stop_words)\n",
    "\n",
    "tfidf_vectorized_phrases = tfidf_vectorizer.fit_transform(phrases)\n",
    "print(f\"Phrase: {phrases[3]}\")\n",
    "print(f\"Vector representation: {tfidf_vectorized_phrases.todense()[3]}\")\n",
    "print(f\"Features: {tfidf_vectorizer.get_feature_names()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### zadanie samodzielne\n",
    "\n",
    "Wykorzystując zbudowany powyżej model językowy spróbuj wyznaczyć reprezentację wektorową poniższego zdania:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T14:25:12.424644Z",
     "start_time": "2020-12-03T14:25:12.409787Z"
    }
   },
   "outputs": [],
   "source": [
    "doc = 'Na terenie Poznania od X wieku nad Wartą znajdowało się centrum wielkopolski'\n",
    "\n",
    "vectorized_doc = # uzupełnij\n",
    "\n",
    "print(vectorized_doc.todense())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
